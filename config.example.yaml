# Augment-BYOK-Proxy (Rust) 配置示例

server:
  # 管理台：http://127.0.0.1:8317/admin（运行时编辑/保存配置）
  # 注意：管理台会显示 token/api_key，建议仅监听 127.0.0.1
  host: "127.0.0.1"
  port: 8317

proxy:
  # VS Code 设置：augment.advanced.apiToken（用于连接本代理的鉴权 token，不是 LLM key）
  auth_token: "proxy_your_auth_token"

official:
  # Augment 官方 completionURL（严格语义：视为完整 API 前缀，不补/抽/猜）
  base_url: "https://api.augmentcode.com/"
  # Augment 官方 token（用于本代理反代到官方；不会暴露给 VS Code；支持 raw token / Bearer / KEY=VALUE）
  api_token: "AUGMENT_API_TOKEN=your_official_token"

byok:
  # 可选：默认 provider（不填则按 providers[0]）
  active_provider_id: "anthropic"
  providers:
    - type: "anthropic"
      id: "anthropic"
      # 严格语义：base_url 视为完整 API 前缀，不补/抽/猜 /v1
      # 例如官方应为：https://api.anthropic.com/v1
      base_url: "https://api.anthropic.com/v1"
      api_key: "sk-ant-your-api-key"
      default_model: "claude-sonnet-4-20250514"
      max_tokens: 8192
      timeout_seconds: 120
      thinking:
        enabled: true
        budget_tokens: 10000
      # 需要时可加 beta header，例如：
      # extra_headers:
      #   anthropic-beta: "thinking-2024-10-22"
      extra_headers: {}

    - type: "openai_compatible"
      id: "openai"
      # 严格语义：base_url 视为完整 API 前缀，不补/抽/猜 /v1
      # 例如 OpenAI 官方应为：https://api.openai.com/v1
      base_url: "https://api.openai.com/v1"
      api_key: "sk-your-openai-api-key"
      default_model: "gpt-4o-mini"
      max_tokens: 8192
      timeout_seconds: 120
      extra_headers: {}

history_summary:
  # 代理侧“自动上下文压缩/摘要”（结合 Augment 的 Summary 模板 + 专用摘要模型）
  # - enabled=false 时完全不生效
  # - enabled=true 时会在 chat_history 过大时额外调用一次“专用摘要模型”生成 summary_text
  enabled: false
  # VSIX 侧“客户端预压缩”（更彻底：在发给 proxy 之前先把 chat_history 压成 summary+tail，避免全量 history 传输）
  # - false：仅由 proxy 侧压缩（client 仍发全量 chat_history）
  # - true：注入版 VSIX 会读取 /admin/api/config 并按同一套阈值做 client-side compaction（需要安装注入版 VSIX）
  client_compaction_enabled: false
  # 专用摘要模型所在 provider（可选；留空则默认用 byok.active_provider_id / providers[0]）
  provider_id: "openai"
  # 专用摘要模型名称（可选；留空则默认用该 provider 的 default_model）
  model: "gpt-4o-mini"
  # 摘要调用 max_tokens / 超时
  max_tokens: 1024
  timeout_seconds: 60
  # 触发策略：
  # - auto：优先按“上下文百分比”触发（需能推断/配置主对话模型 context window），否则回退为字符阈值
  # - chars：仅按字符阈值触发（与官方近似）
  # - ratio：仅按上下文百分比触发（无法推断 context window 时会回退为 chars）
  trigger_strategy: "auto"
  # 上下文百分比触发阈值/目标阈值：超过 trigger 会触发压缩；压缩会尽量收敛到 target（保留 min_tail_exchanges 前提下）
  trigger_on_context_ratio: 0.70
  target_context_ratio: 0.55
  # 主对话模型的 context window（tokens）：0 表示未知；建议在服务端部署时按你的主模型族设置
  # 例：OpenAI 常见 128000；Anthropic 常见 200000；Gemini 2.5 Pro 常见 1000000
  context_window_tokens_default: 0
  # 针对不同主对话模型的覆盖（key 为匹配子串；优先级高于 default）
  context_window_tokens_overrides:
    "gemini-2.5-pro": 1000000
    "claude-": 200000
    "gpt-4o": 128000
  # 触发阈值（字符估算，参考官方 dK≈800000）：超过后对旧 history 做摘要并裁剪
  trigger_on_history_size_chars: 800000
  # 尾部保留预算（避免切断近期上下文/工具链）：最后这段不参与摘要，仍以原始 exchange 形式保留
  history_tail_size_chars_to_exclude: 250000
  # 尾部至少保留多少个 exchange（建议 >=2，避免 tool_result 孤儿）
  min_tail_exchanges: 2
  # 代理侧摘要缓存（按 conversation_id 记忆同一边界的 summary_text，减少重复调用）
  cache_ttl_ms: 1800000
  # 摘要输入最大长度（防止摘要模型被超长输入拖垮）
  max_summarization_input_chars: 250000
  # 滚动摘要：首次全量摘要，后续只对“新增片段”做增量更新（降低摘要调用成本）
  rolling_summary: true
  # 摘要 prompt：默认内置 Codex 风格；如需更强约束可在此覆盖
  # prompt: |
  #   ...

logging:
  # tracing filter（tracing_subscriber EnvFilter 语法），默认 info
  filter: "info"
  # 排查 /chat-stream 请求：输出已脱敏摘要 JSON（默认省略 prefix/suffix/rules/tool_definitions/nodes/chat_history/blobs；不截断）
  dump_chat_stream_body: false
